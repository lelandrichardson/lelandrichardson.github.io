{
  "data": {
    "site": {
      "siteMetadata": {
        "title": "Intelligible Babble",
        "author": "Leland Richardson"
      }
    },
    "markdownRemark": {
      "id": "7afaa670-11e9-5e28-b8ad-4924ffd71af5",
      "excerpt": "In the world of search and text indexing, Lucene is perhaps the most pervasive implementation. These days many developers opt to use…",
      "html": "<p>In the world of search and text indexing, Lucene is perhaps the most pervasive implementation. These days many developers opt to use technology that uses Lucene’s inverted index implementation under-the-hood, like Elastic Search, Solr, etc.</p>\n<p>Indexing text is hard. Lucene will get you most of the way, but massive search engines like Google have spoiled most of the public into expecting search to “just work”.</p>\n<p>At Tech.pro, we use Lucene.Net to power our search, and it works pretty well, but there are some issues.  One issue in particular that I’d like to talk about today is tokenizing.</p>\n<p>Lucene has some built in tokenizers that work pretty well for most scenarios. Tokenizers are meant to take in an arbitrary piece of text content, and then normalize it into a stream of “tokens” which can then be compared to one another, where equality would mean that they “meant” the same thing.</p>\n<p>This seems fairly simple, and it can be, but it can also get quite complicated.</p>\n<p>For instance, consider the simple situation of plural word forms:</p>\n<blockquote>\n<p>The cat was cleaning its fur</p>\n</blockquote>\n<p>versus</p>\n<blockquote>\n<p>The cats were cleaning their fur</p>\n</blockquote>\n<p>In this case, if we made the search <code class=\"language-text\">cat fur</code>, depending on your tokenizer, only the first example, with the non-plural “cat” would show up.</p>\n<p>Lucene comes with several tokenizers to abstract out some of this complexity for common use cases. In Tech.pro’s case, we used the <code class=\"language-text\">StandardAnalyzer</code>, which is intended for text in the english language, and normalizes based on a couple of factors.</p>\n<p>One thing the standard analyzer does is remove punctuation and non-word characters.  This is normally desirable, but on Tech.pro we talk a lot about technologies, some of which have punctuation in their name.</p>\n<p>Consider the scenario where someone wants to search for articles on C#.  They might search for <code class=\"language-text\">C# tutorial</code>, which lucene would then tokenize into the two tokens: <code class=\"language-text\">c tutorial</code>.</p>\n<p>The problem here is that <code class=\"language-text\">C#</code> gets tokenized into <code class=\"language-text\">c</code>, but <code class=\"language-text\">c</code> is already a meaningful think in tech (a programming language), not to mention that there is also <code class=\"language-text\">C++</code> (another language) which also gets normalized into <code class=\"language-text\">c</code>.</p>\n<p>This had been a known problem in our search since day one, so we decided to investigate to see what could be done.</p>\n<h3>Creating a custom Tokenizer</h3>\n<p>I didn’t want to reinvent the wheel, so from the get-go I tried to figure out what classes Lucene already exposes that we could override for our purposes.</p>\n<p>The <code class=\"language-text\">CharTokenizer</code> was one such class. The <code class=\"language-text\">CharTokenizer</code> is a very basic implementation of a tokenizer which simply looks character-to-character to determine if a character is or isn’t part of a token, and then streams contiguous token characters together into a single token.</p>\n<p>This works great if determining whether or not a character is part of a token is only a function of the character itself.  This isn’t the scenario we are in, but it is a useful first step.  I created a <code class=\"language-text\">TechKeywordTokenizer</code> that simply includes all letters, digits, and the characters <code class=\"language-text\">+</code>, <code class=\"language-text\">#</code>, and <code class=\"language-text\">.</code> into the set of valid token characters. After some looking around, this seems to be inclusive of most of the tech keywards that I wanted to differentiate.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">public class TechKeywordTokenizer : CharTokenizer\n{\n    protected override char Normalize(char c)\n    {\n        return char.ToLower(c);\n    }\n\n    protected override bool IsTokenChar(char c)\n    {\n        // we are splitting tokens on non-letters, so long\n        // as they are not &quot;+&quot; or &quot;#&quot;, which are often used in tech\n        // key words (like C++ and C#)\n        return char.IsLetterOrDigit(c) || c == &#39;+&#39; || c == &#39;#&#39; || c == &#39;.&#39;;\n    }\n}</code></pre></div>\n<p>Stopping here would clearly be problematic, as now we’ve just lobbed a huge curve-ball into our indexing process. At this point, if a word was at the end of a sentence, it would tokenize differently than if it was in the middle of a sentence. For instance, with the two sets of text:</p>\n<blockquote>\n<p>This was fun.</p>\n</blockquote>\n<p>and </p>\n<blockquote>\n<p>This was fun and scary.</p>\n</blockquote>\n<p>Only the latter would show up for a search for <code class=\"language-text\">fun</code>.</p>\n<p>In this case, we need to create a <code class=\"language-text\">TokenFilter</code> to take in the tokens from the <code class=\"language-text\">TechKeywordTokenizer</code>, and let the good ones through, and fix the bad ones (like periods at the end of a sentence).</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">public class TechKeywordTokenFilter : TokenFilter\n{\n    private TermAttribute termAtt;\n\n    public TechKeywordTokenFilter(TokenStream input) : base(input)\n    {\n        this.termAtt = (TermAttribute)this.AddAttribute(typeof(TermAttribute));\n    }\n\n    private string Normalize(string term)\n    {\n        term = term.TrimStart(&#39;#&#39;, &#39;+&#39;);\n        term = term.TrimEnd(&#39;.&#39;);\n        if (term.Length == 0)\n        {\n            return null;\n        }\n        return term;\n    }\n\n    public override bool IncrementToken()\n    {\n        while (this.input.IncrementToken())\n        {\n            var term = this.termAtt.Term();\n            var first = term[0];\n            var last = term[term.Length - 1];\n\n            // unless these conditions are met, we don&#39;t need to do\n            // anything special\n            if (first == &#39;#&#39; || first == &#39;+&#39; || last == &#39;.&#39;)\n            {\n                // update the term to what we actually want it to be.\n                // if null is returned, then move onto next term\n                var newTerm = Normalize(termAtt.Term());\n\n                if (newTerm == null)\n                {\n                    continue; // next token\n                }\n                termAtt.SetTermBuffer(newTerm);\n            }\n\n            return true;\n        }\n        return false;\n    }\n\n}</code></pre></div>\n<p>This is a lot of code for something pretty simple. The important stuff is all in the <code class=\"language-text\">Normalize</code> function:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">private string Normalize(string term)\n{\n    term = term.TrimStart(&#39;#&#39;, &#39;+&#39;);\n    term = term.TrimEnd(&#39;.&#39;);\n    if (term.Length == 0)\n    {\n        return null;\n    }\n    return term;\n}</code></pre></div>\n<p>Essentially, our tokenizer has left some special characters like <code class=\"language-text\">+</code>, <code class=\"language-text\">.</code>, and <code class=\"language-text\">#</code> on our tokens, but we are removing them again in some cases. Namely, we are removing <code class=\"language-text\">+</code> and <code class=\"language-text\">#</code> from the start of terms, and <code class=\"language-text\">.</code> from the end of terms. (that’s it!).</p>\n<h3>Building our Analyzer</h3>\n<p>We now want to build an analyzer that does the full tokenization of our text. This is simply putting together the classes we have just built, and the built-in tokenizers/filters that we want to use on top of our own.</p>\n<p>In this case we want to also add the english <code class=\"language-text\">StopFilter</code> to our overall analysis. To do this, we create a new analyzer and implement the <code class=\"language-text\">TokenStream</code> method:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">public class TechKeywordAnalyzer : Analyzer\n{\n    public override TokenStream TokenStream(string fieldName, TextReader reader)\n    {\n        TokenStream result = new TechKeywordTokenizer(reader);\n        result = new TechKeywordTokenFilter(result);\n        result = new StopFilter(false, result, StopAnalyzer.ENGLISH_STOP_WORDS_SET, true);\n        return result;\n    }\n}</code></pre></div>\n<p>and that’s it!</p>\n<h3>Have anything to add?</h3>\n<p>I did a lot of searching to see how others have tackled problems like this, and wound up with very little. If you’ve implemented similar things for similar problems, or have anything you think we should add to this implementation, let me know!</p>",
      "fields": { "readingTime": { "text": "6 min read" } },
      "frontmatter": {
        "title": "Custom Lucene Tokenizer for Tech Keywords",
        "date": "May 13, 2015"
      }
    }
  },
  "pageContext": {
    "slug": "/custom-lucene-tokenizer-for-tech-keywords/",
    "previous": {
      "fields": {
        "slug": "/jason-calacanis--epic-rant-on-the-state-of-the-valley/"
      },
      "frontmatter": {
        "title": "Jason Calacanis' Epic Rant on the State of the Valley"
      }
    },
    "next": {
      "fields": { "slug": "/making-reactjs-and-knockoutjs-play-nice/" },
      "frontmatter": { "title": "Making React.js and Knockout.js Play Nice" }
    }
  }
}
